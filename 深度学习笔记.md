# 1. 介绍

## 1.1 Anaconda

Anaconda是一个包含数据科学常用包的 Python 发行版本。它基于 conda ——一个包和环境管理器——衍生而来。你将使用 conda 创建环境，以便分隔使用不同 Python 版本和不同程序包的项目。你还将使用它在环境中安装、卸载和更新包。

## 1.2 conda

conda是一个环境管理器，Conda是在Windows、mac和Linux上运行的开源软件包管理系统和环境管理系统

Conda可以轻松地在本地计算机上的环境和版本中创建，保存，加载和切换

例如：在我们做大型项目中，需要3.5版本的python环境时，利用conda可以配置需要的环境，当需要3.8版本环境时，也可以直接用conda进行环境配置。方便我们在之后更方便


# 2. 终端

如图在pycharm中的Terminal也可以进行类似cmd的操作。

![image-20230512220525451](./深度学习笔记.assets/image-20230512220525451.png)

## 2.1 conda

查看python版本

```
python
python --version
```

测试conda是否正常

```cmd
conda
```

查看conda版本

```
conda -V
conda --version
```

进入base环境

```
activate
```

创建环境（创建3.8版本名为xxx的环境）

```
conda create -n xxx python=3.6

eg：conda create -n python2 python=python2.7 numpy pandas
#创建了python2环境，python版本为2.7，同时还安装了numpy pandas包
```

 选择环境（名为xxx的环境）

```
conda activate xxx
```

退出到base环境

```
conda deactivate
```

删除某个环境

```
conda remove -n env_name --all
conda env remove -n env_name    	#上面失败时用
conda remove package       			#删除当前环境中的包
conda remove -n env_name  package   #删除指定环境中的包
```

查看当前所有环境

```
conda  env  list
```

查看当前环境的所有安装包

```
conda  list  //需进入该虚拟环境
conda  list  -n  env_name
```

安装or卸载包

```
conda  install  xxx
conda  install  xxx=版本号      		  	   # 指定版本号
conda  install  xxx -i 源名称或链接      		 # 指定下载源，相对上面2行更快
conda  install --name env_name package_name  #在指定环境中安装包
conda  uninstall  xxx
```

conda和pip的数据源

```
1、conda数据源管理：
#显示目前conda的数据源有哪些
conda config --show channels
#添加数据源：例如, 添加清华anaconda镜像：
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
conda config --set show_channel_urls yes
#删除数据源
conda config --remove channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
#显示目前pip的数据源有哪些
pip config list
pip config list --[user|global] # 列出用户|全局的设置
pip config get global.index-url # 得到这key对应的value 如：https://mirrors.aliyun.com/pypi/simple/
#添加数据源：例如, 添加USTC中科大的源：
pip config set global.index-url https://mirrors.ustc.edu.cn/pypi/web/simple
#添加全局使用该数据源
pip config set global.trusted-host https://mirrors.ustc.edu.cn/pypi/web/simple
# 删除
pip config unset key
————————————————
tips：
#本人的 ~/.condarc
auto_activate_base: false
channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
    show_channel_urls: true
```

## 1.3 cuda







# 3. IDE

## 3.1 pycharm

### 3.1.1 pytharm添加解释器

选择conda安装路径的python.exe

![image-20230512222231529](./深度学习笔记.assets/image-20230512222231529.png)



## 3.2 编辑器Jupyter





# 4. 常用库(函数)

## 4.1 os库

### 4.1.1 os.path

os.path 模块是系统路径操作模块，其中，斜杠("/")是 linux 系统下的路径分隔符，和反斜杠("\")是 windows 系统下的路径分隔符。

也就是说，只要提供一个包含斜杠和反斜杠的字符串，os.path 模块都能处理，哪怕该字符串不是一个有效的真正路径，因为 os.path 模块的源码实现就是根据操作系统来处理斜杠和反斜杠的操作的。

#### os.path.join**(path1, path2, ...)**

路径合并函数，这个函数会把所有参数合并成一个路径字符串，其中除了最后一个参数之外，其它所有参数都会自动在字符串末尾添加目录分隔符(斜杠或者反斜杠)，linux系统下默认添加斜杠，windows下默认添加一个反斜杠

### 4.1.2 os模块常用函数

#### [os.listdir(path)](https://www.runoob.com/python3/python3-os-listdir.html) 

返回path指定的文件夹包含的文件或文件夹的名字的list。

```python
import os
path = "dataset\\train\\ants"
img_path_list = os.listdir(path) 
```



## 4.2 [Pillow(PIL)](https://blog.csdn.net/weixin_43790276/article/details/108478270?ydreferer=aHR0cHM6Ly9jbi5iaW5nLmNvbS8%3D)

#### 安装 pip install pillow

```python
#导包时要用PIL来导入，而不能用pillow或Pillow
import PIL 或
from PIL import Image
```

#### Image.open() 、Image.show() 

```python
from PIL import Image
 
image = Image.open("yazi.jpg")
image.show()
```



## 4.3 pytorch

*pytorch可以说是torch的python版，并增加了很多新功能*

*安装pytorch*：

进入pytorch官网选择对应配置进行安装

```py
#进入对应虚拟环境
conda activate xxx
#独显安装：
#这里若python为3.6，安装1.8.1
conda install pytorch==1.8.1 torchvision==0.9.1 torchaudio==0.8.1 cudatoolkit=10.2
conda install pytorch torchvision torchaudio pytorch-cuda=11.7

#核显安装选cpu=none选项

#检测安装是否成功
import torch 
print(torch.__version__)    #显示版本
print("gpu", torch.cuda.is_available())  #gpu TRUE
```



### 4.3.1 [Dataset、DataLoader](https://blog.csdn.net/weixin_44211968/article/details/123744513?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169405322416800186594999%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169405322416800186594999&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-123744513-null-null.142^v93^insert_down28v1&utm_term=torch%20Dataset&spm=1018.2226.3001.4187)

训练模型一般都是先处理 **数据的输入问题** 和 **预处理问题** 。Pytorch提供了几个有用的工具：torch.utils.data.Dataset 类和 torch.utils.data.DataLoader 类 。

流程是先把原始数据转变成 torch.utils.data.Dataset 类，随后再把得到的 torch.utils.data.Dataset 类当作一个参数传递给 torch.utils.data.DataLoader 类，得到一个数据加载器，这个数据加载器每次可以返回一个 Batch 的数据供模型训练使用。

在 pytorch 中，提供了一种十分方便的数据读取机制，即使用 torch.utils.data.Dataset 与 Dataloader 组合得到数据迭代器。在每次训练时，利用这个迭代器输出每一个 batch 数据，并能在输出时对数据进行相应的预处理或数据增广操作。

#### Dataset

Dataset 是一个 **数据集** 抽象类，它是其他所有数据集类的父类（所有其他数据集类都应该继承它），继承时需要重写方法 `__len__` 和 `__getitem__` ， `__len__` 是提供数据集大小的方法， `__getitem__` 是可以通过索引号找到数据的方法。

以下是一个简单的自定义数据集类重写demo

```py
class MyDataset(Dataset):
    root_dir = ''
    label_dir = ''
    path = ''
    img_path_list = ''
    
    def __init__(self,root_dir,label_dir):
        ...
    
    def __getitem__(self,idx):
        img_name = self.img_path_list[idx]
        img_item_path = os.path.join(...)
    	img = Image.open(img_item_path)
        label = self.label_dir
        return img,label
    
    def __len__(self):
        return len(img_path_list)
```

​			

## 4.4 torch.utils.tenserBoard

### 4.4.1 介绍

TensorBoard 是一组用于[数据可视化)的工具。TensorBoard 包含在 TensorFlow 库中，可以直接用，但在pytorch中不行，pytorch要单独安装 TensorBoard 可以使用**pip install tensorboard**

安装

```py
pip install tensorboard
```



### 4.4.2 基本使用

#### 1 标量展示writer.add_scalar

```py
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter("logs")
#y=x
for i in range(100):
    writer.add_scalar("y=2x",2i,i)  #展示y=2x函数图像

writer.close()
```

#### 2 图像展示add_image

```py
from torch.utils.tensorboard import SummaryWriter            tensorboard可视化库
import numpy as np								PIL类型不能直接被writer读取，转成numpy
from PIL import Image							用于导入图片

writer = SummaryWriter("logs")

image_path="D:\Pytorch_learn_project\data\\train\\ants_image\\24335309_c5ea483bb8.jpg"   图片路径
img_PIL= Image.open(image_path)         根据上述路径打开图片
img_array = np.array(img_PIL)			转为numpy类型，方便按照numpy类型保存图片的数据信息

#使用writer.add_image()函数展示图片，最后关闭writer()，这里不是默认的CHW形状，这里需记得标明数据的形状
writer.add_image("test1",img_array,2,dataformats='HWC')#2是步长
writer.close()

```

#### 3 终端启用tensorboard

```py
#在终端输入命令启用tensorboard的web端
#记住一定要在对应文件的上级目录的终端
tensorboard --logdir=要保存的目标目录 --port=6007
```



## 4.5 torch.tensor

在[神经网络](https://so.csdn.net/so/search?q=神经网络&spm=1001.2101.3001.7020)的计算中，数据都是以tensor（张量）的形式进行传递和运算的.
tensor是对一类数学概念的一个概括：

- 0维tensor = 数字 = 标量
- 1维tensor = 序列 = 向量
- 2维tensor = 2维序列 = 矩阵
- n维tensor = n维序列

其中n也代表了访问tensor中某个元素所需要的indexs的数量，例如对于一个2维的tensor:

> a = [
> [1, 2],
> [3, 4]
> ]

当我们想要访问3这个元素时候需要输入:a[1] [0] 得到3

可以看到，需要2个indexs.

Tensor中除了数据，还包装了神经网络中如梯度等的一些基本属性，所以说数据在神经网络中一定要转换成tensor型再进行训练

![image-20230911193346920](./深度学习笔记.assets/image-20230911193346920.png)

### 4.5.1 tensor与Tensor的区别

首先，torch.Tensor是一个类，所有的tensor都是Tensor的一个实例；而torch.tensor是一个函数。这也说明了为什么使用torch.Tensor()没有问题而torch.tensor()却有问题。
其次，torch.tensor主要是将一个data封装成tensor，并且可以指定requires_grad。
torch.tensor(data,dtype=None,device=None,requires_grad=False) - > Tensor
最后，我们更多地使用torch.tensor，我们可以通过使用torch.tensor(())来达到与torch.Tensor()同样的效果。

```py
import torch
import numpy as np

a = torch.Tensor([2,3])
print(a.dtype)  # torch.floaat32

b = torch.tensor([2,3])
print(b.dtype)  # torch.int64

c = np.array(2,3)
print(c.dtype) # int64
```

### 4.5.2 tensor的一些操作

#### 1 cat()

```py
#实现拼接
b=torch.randn(3,3)
print(b)
tensor([[-0.0733, -0.4884,  0.9050],
        [-1.2613,  0.8577,  0.2139],
        [ 0.3336, -1.9796, -0.1387]])

e=torch.cat((b,b), dim=0) #dim=0是按行拼接
>>>e
tensor([[-0.0733, -0.4884,  0.9050],
        [-1.2613,  0.8577,  0.2139],
        [ 0.3336, -1.9796, -0.1387],
        [-0.0733, -0.4884,  0.9050],
        [-1.2613,  0.8577,  0.2139],
        [ 0.3336, -1.9796, -0.1387]])


d=torch.cat((b,b),dim=1)  #维度为1是按列拼接
print(d)
tensor([[-0.0733, -0.4884,  0.9050, -0.0733, -0.4884,  0.9050],
        [-1.2613,  0.8577,  0.2139, -1.2613,  0.8577,  0.2139],
        [ 0.3336, -1.9796, -0.1387,  0.3336, -1.9796, -0.1387]])
```







## 4.6 torchvision/torch

torchvision 工具库是 pytorch 框架下常用的图像视频处理包，可以用来生成图片和视频数据集（torchvision.datasets），做一些图像预处理(torchvision.transforms)，导入预训练模型(torchvision.models)，以及生成和保存图像（torchvision.utils）。

### 4.6.1 transforms

transforms函数对图像做预处理可以是：归一化(normalize)，尺寸剪裁(resize)，翻转(flip) 等。

上面的这些步骤实际操作起来往往是一系列的，此时可以用compose将这些图像预处理操作连起来。

#### 1 transforms.ToTensor

````py
- 可以将PIL和numpy.ndarry格式的数据从[0,255]范围转换到[0,1] ，其实内部就是将原始数据直接除以255。
- 另外原始数据的shape是（H x W x C），通过`transforms.ToTensor()`后shape会变为（C x H x W）。

```py
import cv2
import numpy as np
import torch
from torchvision import transforms

#原始数据模型
data = np.array([
                [[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]],
                [[2,2,2],[2,2,2],[2,2,2],[2,2,2],[2,2,2]],
                [[3,3,3],[3,3,3],[3,3,3],[3,3,3],[3,3,3]],
                [[4,4,4],[4,4,4],[4,4,4],[4,4,4],[4,4,4]],
                [[5,5,5],[5,5,5],[5,5,5],[5,5,5],[5,5,5]]
        ],dtype='uint8')

#toTensor转换
data = transforms.ToTensor()(data)

#输出
print(data)
print(data.shape) #torch.Size([3, 5, 5])CHW
--------------------------------------------------------
#1/255=0.0039
tensor([[[0.0039, 0.0039, 0.0039, 0.0039, 0.0039],
         [0.0078, 0.0078, 0.0078, 0.0078, 0.0078],
         [0.0118, 0.0118, 0.0118, 0.0118, 0.0118],
         [0.0157, 0.0157, 0.0157, 0.0157, 0.0157],
         [0.0196, 0.0196, 0.0196, 0.0196, 0.0196]],
        .......
         [[0.0039, 0.0039, 0.0039, 0.0039, 0.0039],
         [0.0078, 0.0078, 0.0078, 0.0078, 0.0078],
         [0.0118, 0.0118, 0.0118, 0.0118, 0.0118],
         [0.0157, 0.0157, 0.0157, 0.0157, 0.0157],
         [0.0196, 0.0196, 0.0196, 0.0196, 0.0196]]])
```

#ToTensor功能是将 PIL Image 类型 或者numpy.ndarray类型的图片对象转换为 tensor类型。from torchvision import transforms

#这是将PILimage类型转换成tensor类型，直接用transforms的转换工具
from PIL import Image
from torchvision import transforms

img_path = "testdata/train/ants_image/6743948_2b8c096dda.jpg"
img = Image.open(img_path)
tensor_tool = transforms.ToTensor()
tensor_img = tensor_tool(img)
print(tensor_img)

#这先得到numpy.ndarry类型再转换成tensor类型，得到ndarry类型需要使用opencv（cv2）
````

#### 2 [transforms.Normalize](https://blog.csdn.net/qq_40507857/article/details/116600119?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169448288016800227484660%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169448288016800227484660&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-116600119-null-null.142^v93^insert_down28v1&utm_term=transforms.normalize&spm=1018.2226.3001.4187&ydreferer=aHR0cHM6Ly9zby5jc2RuLm5ldC9zby9zZWFyY2g%2Fc3BtPTEwMDEuMjEwMS4zMDAxLjQ0OTgmcT10cmFuc2Zvcm1zLm5vcm1hbGl6ZSZ0PSZ1PQ%3D%3D)

![image-20220416200305159](https://img-blog.csdnimg.cn/img_convert/c9ac9390e825a69f6af3763baac8852e.png#pic_center)

可以看到这个函数的输出`output[channel] = (input[channel] - mean[channel]) / std[channel]`。这里[channel]的意思是指对特征图的每个通道都进行这样的操作。【mean为均值，std为标准差】

这里的第一个参数（0.5，0.5，0.5）表示每个通道的均值都是0.5，第二个参数（0.5，0.5，0.5）表示每个通道的方差都为0.5。【因为图像一般是三个通道，所以这里的向量都是1x3的🍵🍵🍵】有了这两个参数后，当我们传入一个图像时，就会按照上面的公式对图像进行变换。【**注意：这里说图像其实也不够准确，因为这个函数传入的格式不能为PIL Image，我们应该先将其转换为Tensor格式**】说了这么多，那么这个函数到底有什么用呢？我们通过前面的ToTensor已经将数据归一化到了0-1之间，现在又接上了一个Normalize函数有什么用呢？其实Normalize函数做的是将数据变换到了[-1,1]之间。之前的数据为0-1，当取0时，output =（0 - 0.5）/ 0.5 = -1；当取1时，output =（1 - 0.5）/ 0.5 = 1。这样就把数据统一到了[-1，1]之间了🌱🌱🌱那么问题又来了，数据统一到[-1，1]有什么好处呢？数据如果分布在(0,1)之间，可能实际的bias，就是神经网络的输入b会比较大，而模型初始化时b=0的，这样会导致神经网络收敛比较慢，经过Normalize后，可以加快模型的收敛速度。【这句话是再网络上找到最多的解释，自己也不确定其正确性】

  读到这里大家是不是以为就完了呢？这里还想和大家唠上一唠🍓🍓🍓上面的两个参数（0.5，0.5，0.5）是怎么得来的呢？这是根据数据集中的数据计算出的均值和标准差，所以往往不同的数据集这两个值是不同的
————————————————

经过上面normalize()的变换后变成了均值为0 方差为1（其实就是最大最小值为1和-1）

每个样本图像变成了均值为0 方差为1 的标准正态分布，这就是最普通（科学研究价值最大的）的样本数据了



#### 3 [transforms.Resize()](https://blog.csdn.net/qq_35008185/article/details/118224044?ops_request_misc=&request_id=&biz_id=102&utm_term=transforms.resize&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-118224044.nonecase&spm=1018.2226.3001.4187)

调整**PILImage对象**的尺寸，或结合Compose([...])使用

> 提示：不能是用**io.imread或者cv2.imread**读取的图片，这两种方法得到的是**ndarray**。

```py
#同时指定长宽
transforms.Resize([h, w])
#将图片缩放至x，长宽比保持不变
transforms.Resize(x)
```



#### 4 [transforms.RandomCrop()](https://blog.csdn.net/u011995719/article/details/85107009?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169460376016800197090384%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169460376016800197090384&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-85107009-null-null.142^v93^insert_down28v1&utm_term=transforms.randomcrop%28%29&spm=1018.2226.3001.4187)

```py
class torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode=‘constant’)
'''
功能：依据给定的size随机裁剪
参数：
size- (sequence or int)，若为sequence,则为(h,w)，若为int，则(size,size)

padding-(sequence or int, optional)，此参数是设置填充多少个pixel。
当为int时，图像上下左右均填充int个，例如padding=4，则上下左右均填充4个pixel，若为3232，则会变成4040。
当为sequence时，若有2个数，则第一个数表示左右扩充多少，第二个数表示上下的。当有4个数时，则为左，上，右，下。

fill- (int or tuple) 填充的值是什么（仅当填充模式为constant时有用）。int时，各通道均填充该值，当长度为3的tuple时，表示RGB通道需要填充的值。

padding_mode- 填充模式，这里提供了4种填充模式，1.constant，常量。2.edge 按照图片边缘的像素值来填充。				  3.reflect，暂不了解。 4. symmetric，暂不了解。
'''
```

————————————————

#### 5 transforms.Compose()

一般用Compose**把多个步骤整合到一起**：

```py
transforms.Compose([
 
    transforms.CenterCrop(10),
    transforms.ToTensor(),
 
])

#也可以提前定义再直接放进去
trans_toTensor = transforms.ToTensor()
trans_crop = transforms.CenterCrop(10)
transforms.Compose([trans_crop, trans_toTensor])
```





### 4.6.2 torchvision.datasets

#### COCO：目标检测，语义分割

#### MNIST：手写文字

#### CIFAR：物体识别

```py
CIFAR10的10个classes：airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck
```

```py
torchvision.datasets.CIFAR10(root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False)
root:目录
transform：是否需要transforms处理图片
download若为true，则会从网上自动下载数据集，否则要自己下载，一般设置为true
```

```py
import torchvision
from torch.utils.tensorboard import SummaryWriter

dataset_transform = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
])

#datasets若发现数据集已经下载并校验通过，则不会重新下载
train_set = torchvision.datasets.CIFAR10(root="./dataset", train=True, transform=dataset_transform, download=True)
test_set = torchvision.datasets.CIFAR10(root="./dataset", train=False, transform=dataset_transform, download=True)


print(test_set[0])#(<PIL.Image.Image image mode=RGB size=32x32 at 0x2176E5FD0C8>, 3)

print(test_set.classes) #airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck

img, target = test_set[0]
print(img)#<PIL.Image.Image image mode=RGB size=32x32 at 0x2541358AA88>
print(target)#3
print(test_set.classes[target])#cat

#画图
writer = SummaryWriter('summary_event')
for i in range(10):
    set_img, target = test_set[i]
    writer.add_image('cifar10', set_img, i)
```



### 4.6.3 torch.nn

#### nn.Module

提供了一些比较常见的神经网络，包括预训练的

自定义神经网络，继承nn.Module时，需要重写init和forward函数

***自定义一个类，继承自Module类，并且一定要实现两个基本的函数，第一是构造函数init，第二个是层的逻辑运算函数，即所谓的前向计算函数forward函数。***

```py
#自定义一个类，继承自Module类，并且一定要实现两个基本的函数，第一是构造函数__init__，第二个是层的逻辑运算函数，即所谓的前向计算函数forward函数。
import torch
from torch import nn

class Tudui(nn.Module):
    def __init__(self):
        super().__init__()
    
    def forward(self, input):
        out = inout + 1
        return output
```



#### nn.functional.conv2d

```py
#输入参数，主要还是input， kernel, stride
def conv2d(input: Tensor, weight: Tensor, bias: Optional[Tensor]=None, 
           stride: Union[_int, _size]=1, padding: Union[_int, _size]=0, 
           dilation: Union[_int, _size]=1, groups: _int=1) -> Tensor: ...，
```

```py
import torch

input = torch.tensor([
    [1, 2, 0, 3, 1],
    [0, 1, 2, 3, 1],
    [1, 2, 1, 0, 0],
    [5, 2, 3, 1, 1],
    [2, 1, 0, 1, 1]
])

kernel = torch.tensor([
    [1, 2, 1],
    [0, 1, 0],
    [2, 1, 0]
])

input = torch.reshape(input, (1, 1, 5, 5))
kernel = torch.reshape(kernel, (1, 1, 3, 3))

torch.nn.functional.conv2d(input, kernel, stride=1)
```

![image-20230914202328633](./深度学习笔记.assets/image-20230914202328633.png)





#### [nn.Conv2d](https://blog.csdn.net/qq_34243930/article/details/107231539?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169474775616800211533990%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169474775616800211533990&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107231539-null-null.142^v94^insert_down28v1&utm_term=nn.CONV2d&spm=1018.2226.3001.4187&ydreferer=aHR0cHM6Ly9zby5jc2RuLm5ldC9zby9zZWFyY2g%2Fc3BtPTEwMDEuMjEwMS4zMDAxLjQ0OTgmcT1ubi5DT05WMmQmdD0mdT0%3D)

```py
#参数详解
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```

output的H,W计算公式

![image-20230915111908894](./深度学习笔记.assets/image-20230915111908894.png)

```py
import torchvision
from torch import nn
from torch.nn import Conv2d
from torch.utils.data import DataLoader

dataset = torchvision.datasets.CIFAR10("dataset", train=False,
                                       transform=torchvision.transforms.ToTensor(), download=True)

dataloader = DataLoader(dataset, batch_size=64)

print(dataset.__len__()) #10000

class Tudui(nn.Module):
    def __init__(self):
        super(Tudui,self).__init__()
        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)

    #在前向传播forward函数里面实现前向运算。
    def forward(self, x):
        x = self.conv1(x)
        return x

tudui = Tudui()

for data in dataloader:
    imgs, targets = data
    output = tudui(imgs)
    print(imgs.shape) #torch.Size([64, 3, 32, 32])
    print(output.shape) #torch.Size([64, 6, 30, 30])--->这里输出通道变成了6，根据上图公式，tensor尺寸也被压缩
    
    #若要进行tensorboard，则需进行reshape，因为图片的显示是3通道
    torch.reshape(output, (-1,3,30,30))
```



#### nn.MaxPool2d

作用:
*对邻域内特征点取最大*
减小**卷积层参数误差**造成估计**均值的偏移**的误差，更多的保留纹理信息。

```py
#参数 一般只填size
MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)

#stride 默认值是 kernel_size
#dilation (int or tuple, optional)【可选】：一个控制窗口中元素步幅的参数
#ceil_mode (bool)【可选】：如果等于True，计算输出信号大小的时候，如果超出范围也会计算，会使用向上取整，代替默认的向下取整的操作
```



#### nn.ReLU/nn.Sigmoid

```py
torch.nn.ReLU(inplace=False)
```



#### [nn.Linear](https://blog.csdn.net/sazass/article/details/123568203?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169508710916800185813549%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169508710916800185813549&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-123568203-null-null.142^v94^insert_down28v1&utm_term=torch%20linear&spm=1018.2226.3001.4187)

Linear运算的本质就是多维（矩阵）的乘法加偏置

```py
torch.nn.Linear(in_features, # 输入的神经元个数
           out_features, # 输出神经元个数
           bias=True # 是否包含偏置
           )
'''
in_feature： int型, 在forward中输入Tensor最后一维(第三位)的通道数

out_feature： int型, 在forward中输出Tensor最后一维的通道数

bias: bool型， Linear线性变换中是否添加bias偏置
'''
```

```py
#示例
X = torch.Tensor([
    [0.1,0.2,0.3,0.3,0.3],
    [0.4,0.5,0.6,0.6,0.6],
    [0.7,0.8,0.9,0.9,0.9],
])
>>>X
tensor([[0.1000, 0.2000, 0.3000, 0.3000, 0.3000],
        [0.4000, 0.5000, 0.6000, 0.6000, 0.6000],
        [0.7000, 0.8000, 0.9000, 0.9000, 0.9000]])
#定义模型-------------------------------------------
model = nn.Linear(in_features=5, out_features=10, bias=True)

model(X).size()#torch.Size([3, 10])
```



#### nn.Sequential

一个序列容器，用于搭建神经网络的模块被按照被传入构造器的顺序添加到nn.Sequential()容器中。

利用nn.Sequential()搭建好模型架构，模型前向传播时调用forward()方法，模型接收的输入首先被传入nn.Sequential()包含的第一个网络模块中。然后，第一个网络模块的输出传入第二个网络模块作为输入，按照顺序依次计算并传播，直到nn.Sequential()里的最后一个模块输出结果。

***这里以如下CIFAR10的一个分类模型为例，将模型进行搭建：***

***可以看出最后生成了一个长度为10的tensor，正好对应该数据集的10个classes***

![image-20230918111430252](./深度学习笔记.assets/image-20230918111430252.png)

```py
from torch import nn
from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear


class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.model = Sequential(
            Conv2d(3, 32, 5, padding=2),
            MaxPool2d(2),
            Conv2d(32, 32, 5, padding=2),
            MaxPool2d(2),
            Conv2d(32, 64, 5, padding=2),
            MaxPool2d(2),
            Flatten(),
            Linear(1024, 64),
            Linear(64, 10)
        )

    def forward(self, x):
        s = self.model(x)
        return x
    
tudui = Tudui()
input = torch.ones(64, 3, 32, 32)
output = tudui(input)
print(output.shape)

#可以通过tensorboard进行网络模型可视化
writer = SummaryWriter('./logs_model')
writer.add_graph(tudui, input)
writer.close()
```



#### nn.L1Loss()

L1Loss 计算方法比较简单，原理就是取预测值和真实值的绝对误差的平均数。计算公式如下

![img](https://img-blog.csdnimg.cn/20191202215436113.png)

```py
#损失函数nn.L1Loss()   作用其实就是计算网络输出与标签之差的绝对值，返回的数据类型可以是张量，也可以是标量。
#参数可以设置取不取平均值
nn.L1Loss(size_average=True, reduce=False,reduction='mean')
```

```py
# output 为网络的输出
# target 为目标输出即对应输入真实的标签
output = torch.ones(2, 3, requires_grad=True)*2.5
target = torch.ones(1, 3)

LOSSresult = nn.L1Loss()
result = LOSSresult(output, target)
 
print('求平均:{}'.format(result))
```



#### **nn.MSELoss**

nn.MSELoss：计算公式是预测值和真实值之间的平方和的平均数。

![img](https://img-blog.csdnimg.cn/20191202215934649.png)

```py
torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')
```

```py

outputs = torch.tensor([1, 2, 3], dtype=torch.float32)
targets = torch.tensor([1, 2, 5], dtype=torch.float32)

outputs = torch.reshape(outputs, (1,1,1,3))
targets = torch.reshape(targets, (1,1,1,3))

loss_mse = MSELoss()
result_mse = loss_mse(inputs, targets)

print(result_mse) #Tensor(1.333)  --->  (0+0+(3-5)^2)/3=4/3=1.333
```



#### nn.CrossEntropyLoss

nn.CrossEntropyLoss是[pytorch](https://so.csdn.net/so/search?q=pytorch&spm=1001.2101.3001.7020)下的**交叉熵损失函数**，用于分类任务使用

> 损失函数的两大作用：
>
> 1. 计算实际输出和目标之间的差距
> 2. 为我们更新输出提供一定的依据（通过反向传播）

计算公式：x就是网络的outputs，可以看出，要检测的对应值概率越大，-x[class]就减的越多，计算出来的损失就越小

![image-20230918170952717](./深度学习笔记.assets/image-20230918170952717.png)



```py
#功能：创建一个交叉熵损失函数：
#一般默认不填参数
torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean', label_smoothing=0.0)
```

```py

loss = nn.CrossEntropyLoss()

tudui = Tudui()

for data in dataLoader:
    imgs, targets = data
    outputs = tudui(imgs)
    #计算每个batch_size输出和标签的损失
    result_loss = loss(outputs, tagets)
    print(result_loss)
```







#### [nn.Softmax()](https://blog.csdn.net/nefetaria/article/details/114411953?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169502603716800185890599%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169502603716800185890599&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-7-114411953-null-null.142^v94^insert_down28v1&utm_term=nn.Softmax%28%29&spm=1018.2226.3001.4187)

```py
tf.nn.softmax(
    logits, # 输入：全连接层（往往是模型的最后一层）的值，一般代码中叫做logits
    axis = None,
    name = None
    dim = None
)
```

作用：softmax函数的作用就是归一化。
输入：全连接层（往往是模型的最后一层）的值，一般代码中叫做logits
输出：归一化的值，含义是属于该位置的概率，一般代码叫做probs,例如输出[0.4, 0.1, 0.2, 0.3],那么这个样本最可能属于第0个位置，也就是第0类。这是由于logits的维度大小就设定的任务的类别，所以第0个位置就代表第0类。softmax函数的输出不改变维度的大小。(该样本属于各个类的概率)
用途：如果做单分类的问题，那么输出的值就取top1(最大, argmax)； 如果做多分类问题，那么输出的值就取topN。



### 4.6.4 [result_loss.backward()](https://blog.csdn.net/sinat_28731575/article/details/90342082?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169503593016800185840467%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169503593016800185840467&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-90342082-null-null.142^v94^insert_down28v1&utm_term=pytorch%20backword&spm=1018.2226.3001.4187)

将损失loss向输入测进行反向传播，得到每个tensor内部要更新参数(optimizer.step())对应的一个梯度参数

```py
result_loss = loss(outputs, targets)
#使用反向传播同时计算梯度
result_loss.backward() #这行运行完后就会更新模型内的grad
```



<img src="https://img-blog.csdnimg.cn/20210405102817343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ2NTEwMjQ1,size_16,color_FFFFFF,t_70" alt="img" style="zoom:67%;" />



### 4.6.5 [torch.norm()](https://blog.csdn.net/qq_36556893/article/details/90698186?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169508938916800225511162%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169508938916800225511162&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-90698186-null-null.142^v94^insert_down28v1&utm_term=torch%20norm&spm=1018.2226.3001.4187)

`torch.norm()` 是 PyTorch 中的一个函数，用于计算输入张量沿指定维度的[范数](https://so.csdn.net/so/search?q=范数&spm=1001.2101.3001.7020)。具体而言，当给定一个输入张量 `x` 和一个整数 `p` 时，`torch.norm(x, p)` 将返回输入张量 `x` 沿着最后一个维度（默认为所有维度）上所有元素的 `p` 范数。

[什么是范数？](https://blog.csdn.net/zhaohongfei_358/article/details/122818616?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169503825316800188522280%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169503825316800188522280&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-122818616-null-null.142^v94^insert_down28v1&utm_term=%E8%8C%83%E6%95%B0&spm=1018.2226.3001.4187)

```py
#参数
def norm(input, p=2, dim=None, keepdim=False, out=None, dtype=None):
    
'''
input：输入tensor类型的数据

p：指定的范数，默认为p=‘fro’，计算矩阵的Frobenius norm (Frobenius 范数)，就是矩阵各项元素的绝对值平方的总和。
p='nuc’时，  是求核范数，核范数是矩阵奇异值的和。（不常用）
p为int的形式，是求p-范数。（常用）

dim：指定在哪个维度进行，如果不指定，则是在所有维度进行计算

keepdim：True or False，如果True，则保留dim指定的维度，False则不保留

out：输出的 tensor

dtype：指定输出的tensor的数据类型
'''
```

```py
tensor([[ 1.,  2.,  3.,  4.],
        [ 2.,  4.,  6.,  8.],
        [ 3.,  6.,  9., 12.]])

#接着我们分别对其行和列分别求2范数
inputs1 = torch.norm(inputs, p=2, dim=1, keepdim=True) #行
print(inputs1)
inputs2 = torch.norm(inputs, p=2, dim=0, keepdim=True) #列
print(inputs2)
#结果：
tensor([[ 5.4772],
        [10.9545],
        [16.4317]])
tensor([[ 3.7417,  7.4833, 11.2250, 14.9666]])
```



### 4.6.6 优化器

#### 1 [torch.optim.SGD](https://blog.csdn.net/echo_gou/article/details/119536350?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169517442516800197019450%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169517442516800197019450&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-119536350-null-null.142^v94^insert_down28v1&utm_term=optim.SGD&spm=1018.2226.3001.4187)

其中的SGD就是optim中的一个算法（优化器）：**随机梯度下降算法**

为了使用torch.optim，你需要构建一个optimizer对象。这个对象能够保持当前参数状态并基于计算得到的梯度进行参数更新。

```py
#参数
optimizer = torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False, *, maximize=False, foreach=None, differentiable=False)
'''
lr (float) – 学习率
momentum (float, 可选) – 动量因子（默认：0）
weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认：0）
dampening (float, 可选) – 动量的抑制因子（默认：0）
nesterov (bool, 可选) – 使用Nesterov动量（默认：False）
'''

#一般使用
tudui = Tudui()
#学习率太小跑得太慢，太大时模型训练不稳定
optimizer = torch.optim.SGD(tudui.parameters, lr=0.01)
```

##### [**优化器的几个方法**](https://blog.csdn.net/weixin_43863869/article/details/128120719?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169517544416800192260220%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169517544416800192260220&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-128120719-null-null.142^v94^insert_down28v1&utm_term=optimizer.zero_grad%28%29%E7%9A%84%E4%BD%9C%E7%94%A8&spm=1018.2226.3001.4187)

```py
optimizer.step()
'''
作用：利用优化器对参数x进行调优，以随机梯度下降SGD为例，更新的公式为：x=x−lr∗(x∗grad)
，lr 表示学习率 lr ，减号表示沿着梯度的反方向进行更新；
'''

optimizer.zero_grad()
'''
作用：清0优化器关于所有参数x的累计梯度值 x∗grad
，一般在loss.backward()前使用
'''
```

一次完整的针对CIFAR10的模型训练，对每次训练迭代进行损失的计算

```py
import torch
import torchvision
from torch import nn
from torch.nn import Sequential, Conv2d, MaxPool2d, Flatten, Linear
#引入数据集和处理器
dataset = torchvision.datasets.CIFAR10('./dataset', train=False,
                                         transform=torchvision.transforms.ToTensor(), download=True)
dataLoader = torch.utils.data.DataLoader(dataset=dataset, batch_size=1)

#创建模型
class Tudui(nn.Module):
    def __init__(self):
        super(Tudui, self).__init__()
        self.model = Sequential(
            Conv2d(3, 32, 5, padding=2),
            MaxPool2d(2),
            Conv2d(32, 32, 5, padding=2),
            MaxPool2d(2),
            Conv2d(32, 64, 5, padding=2),
            MaxPool2d(2),
            Flatten(),
            Linear(1024, 64),
            Linear(64, 10)
        )

    def forward(self, x):
        x = self.model(x)
        return x


tudui = Tudui()
loss = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(tudui.parameters(), lr=0.01)

#开始训练
for epoch in range(3):
    sum_loss = 0.0
    for data in dataLoader:
        imgs, targets = data
        outputs = tudui.forward(imgs)
        #计算损失
        result_loss = loss(outputs, targets)
        #optimizer.zero_grad() 作用：清除之前积累的梯度
        # 一般在loss.backward()前使用，即清除 (x∗grad)pre
        optimizer.zero_grad()
        #反向传播，更新梯度
        result_loss.backward()
        #对每个参数进行调优
        optimizer.step()
        sum_loss += result_loss
    print(sum_loss)
```

```py
#输出
C:\Users\1\miniconda3\envs\pytorchlearn\python.exe D:\pycharmProjects\pytorchlearn\nn_optim.py 
Files already downloaded and verified
tensor(18721.0879, grad_fn=<AddBackward0>)
tensor(16111.2578, grad_fn=<AddBackward0>)
tensor(15386.7744, grad_fn=<AddBackward0>)
```

### 4.6.7 torchvision.models

#### 1  VGG(常用：VGG16/19)

##### 创建模型

```py
torchvision.models.vgg16(*, weights: Optional[VGG16_Weights] = None, progress: bool = True, **kwargs: Any) → VGG
'''
weights:要使用的训练权重,若不使用预训练，则填一个weight=None，若要使用预训练模型，则用默认(weight='VGG16_Weights.DEFAULT')
progress：如果为True，则显示下载到标准程序的进度条。默认为True
一般填这两个就行
'''
```

```py
vgg16_true = torchvision.models.vgg16( weights='DEFAULT')

vgg16_false = torchvision.models.vgg16(weights=None)

print(vgg16_true)
#---------------------------------------------------------
VGG(
  #提取特征
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  #map_location=torch.device('cpu')，意思是映射到cpu上，在cpu上加载模型，无论你这个模型从哪里训练保存的，主要是改变分辨率
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  #分类器，预测用
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
```

##### **修改模型**

```py
#对现有模型添加层
vgg16_true.classifier.add_module('add_Linear', nn.Linear(1000, 10))
print(vgg16_true)
---------------------------------------------
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
    (add_Linear): Linear(in_features=1000, out_features=10, bias=True) #-->添加了一层
  )

#对现有模型修改层
vgg16_false.classifier[6] = nn.Linear(4096, 10)
print(vgg16_false)
----------------------------------------------
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=10, bias=True) #-->被修改了
  )
```

##### 保存模型

```py
vgg16 = torchvision.models.vgg16(weights=None)

#保存方式一
torch.save(vgg16, 'vgg16_method1.pth')

#保存方式二：以字典的形式保存
torch.save(vgg16.state_dict(), 'vgg16_method2.pth')
```

##### [加载模型](https://blog.csdn.net/pearl8899/article/details/109566084?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169529774616800192295944%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169529774616800192295944&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-109566084-null-null.142^v94^insert_down28v1&utm_term=torch.load&spm=1018.2226.3001.4187)

```py
#方式一
model1 = torch.load('vgg16_method1.pth')
# print(model1)

#方式二:以字典方式加载
map_location=torch.device('cpu') #意思是映射到cpu上，在cpu上加载模型，无论你这个模型从哪里训练保存的
model2 = torch.load('vgg16_method2.pth')
print(model2)

#方式二：加载模型
vgg16 = torchvision.models.vgg16(weights=None)
vgg16.load_state_dict(torch.load('vgg16_method2.pth'))
#这样就能把dict存储的模型转回原来的方式输出
print(vgg16)
```

#### 2 model.train()/eval()



## 4.7 opencv

```py
#windows使用pip安装
pip install opencv-python -i https://pypi.tuna.tsinghua.edu.cn/simple
```



# 5 问题

## 5.1 python问题

### 5.1.1 [python数据读取路径为啥要用双反斜杠?](https://www.cnblogs.com/zwt20120701/p/11267457.html)

### 5.1.2 [r'字符串'是什么意思]([Python中的r字符串前缀及其用法详解_python 字符串前带r_念广隶的博客-CSDN博客](https://blog.csdn.net/lsoxvxe/article/details/132001495?ops_request_misc=%7B%22request%5Fid%22%3A%22169405062216777224494582%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&request_id=169405062216777224494582&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-132001495-null-null.142^v93^insert_down28v1&utm_term=python r字符串&spm=1018.2226.3001.4187))

### 5.1.3 Python也有数组，为什么要用Numpy？

Python中提供了list容器，可以当作数组使用。但列表中的元素可以是任何对象，因此列表中保存的是对象的指针，这样一来，为了保存一个简单的列表[1,2,3]。就需要三个指针和三个整数对象。对于数值运算来说，这种结构显然不够高效。
Python虽然也提供了array模块，但其只支持一维数组，不支持多维数组，也没有各种运算函数。因而不适合数值运算。
Numpy内置丰富的方法，能更加轻松高效地进行科学计算。

### 5.1.4 使用各类方法的注意

关于使用各类函数时的注意点：若声明处有默认值，一般不动，填参数时只需填没有默认值的值就可以了

不知道新函数的返回值时，以下几种思路：

> print
>
> print(type(...))
>
> debug
>
> 上网查

### 5.1.5 定义类时可以直接使用self.xx代替变量的声明



## 5.2 数学问题

### 5.2.1 [范数](https://blog.csdn.net/weixin_42066990/article/details/118636995?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169508767416800188597670%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169508767416800188597670&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-118636995-null-null.142^v94^insert_down28v1&utm_term=%E8%8C%83%E6%95%B0%E6%98%AF%E4%BB%80%E4%B9%88&spm=1018.2226.3001.4187)

它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。



# 6 ai理论

## 6.1 [深度学习-图像基础](https://blog.csdn.net/weixin_44211968/article/details/123876506?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169459167716800222837715%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169459167716800222837715&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-123876506-null-null.142^v93^insert_down28v1&utm_term=%E5%9B%BE%E5%83%8F%E9%80%9A%E9%81%93&spm=1018.2226.3001.4187)

图片可以看作是 **三层** **二维数组** 的叠加，每一层二维数组都是一个通道。单通道的图像是灰色的，每个像素pixel只有一个value，数字越高，颜色越白，也就越亮。即三通道，每个通道由二维数组存储。

三层的 value 分别代表着这个点在三个通道的数值，计算机根据这些数值来确定这一个像素点的颜色。这就常见的三层 RGB 色彩空间的工作方式。

RGB 格式里(0,0,0)代表着黑色，(255,255,255)代表着白色。

注意: OpenCV(开源计算机视觉库，包含了许多可用的视觉算法，图像处理必备神器)图像通道的**默认排序是 BGR**。



## 6.2 [归一化（normalization）](https://blog.csdn.net/yangbindxj/article/details/125294045?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169448177316800215060118%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169448177316800215060118&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-2-125294045-null-null.142^v93^insert_down28v1&utm_term=%E5%BD%92%E4%B8%80%E5%8C%96&spm=1018.2226.3001.4187&ydreferer=aHR0cHM6Ly9zby5jc2RuLm5ldC9zby9zZWFyY2g%2Fc3BtPTEwMDEuMjEwMS4zMDAxLjQ0OTgmcT0lRTUlQkQlOTIlRTQlQjglODAlRTUlOEMlOTYmdD0mdT0%3D)

**归一化就是要把图片3个通道中的数据整理到[-1, 1]区间**

为了消除指标之间的量纲影响，需要进行[数据标准化处理](https://so.csdn.net/so/search?q=数据标准化处理&spm=1001.2101.3001.7020)，以解决数据指标之间的可比性。原始数据经过数据标准化处理后，各指标处于同一数量级，适合进行综合对比评价。其中，最典型的就是数据的归一化处理。

**简而言之，归一化的目的就是使得预处理的数据被限定在一定的范围内（比如[0,1]或者[-1,1]），从而消除奇异样本数据导致的不良影响。**

![img](D:\Learn\python-code-note\assets\3e6e5ae01efdc6e97353275439b307de.png)

**奇异样本数据的存在会引起训练时间增大，同时也可能导致无法收敛，因此，当存在奇异样本数据时，在进行训练之前需要对预处理数据进行归一化；反之，不存在奇异样本数据时，则可以不进行归一化。**

**归一化的对比**

--如果不进行归一化，那么由于特征向量中不同特征的取值相差较大，会导致目标函数变“扁”。这样在进行梯度下降的时候，梯度的方向就会偏离最小值的方向，走很多弯路，即训练时间过长。



## 6.3 [HWC和CHW](https://blog.csdn.net/hh1357102/article/details/130622666?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169459146616800227429193%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169459146616800227429193&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-130622666-null-null.142^v93^insert_down28v1&utm_term=RGB%E5%9B%BE%E5%83%8F%E7%9A%84%E5%AD%98%E5%82%A8&spm=1018.2226.3001.4187)

### 6.3.1 HWC

**HWC可以看作是一幅图像的shape**，H表示图像的高度(height)，W表示图像的宽度(Width)，而C表示一幅图像的通道数(Channel)。HWC格式是指按照高度、宽度和通道数的顺序排列图像尺寸的格式。例如，一张形状为256×256×3的RGB图像，在HWC格式中表示为[256, 256, 3]。在一些图像处理库或者底层框架中，例如**OpenCV和TensorFlow，通常使用HWC格式表示图像尺寸。**

在OpenCV中，读取的图片默认是HWC格式

**举个小栗子**

hwc=[5,5,3] 的含义是什么呢？

大家可以思考一下，以图像为例，那么高宽都是5，3就是其中的通道数。因此我们也可以理解为***\*3个 5\*5 大小的特征图\****。 



### 6.3.2 CHW

CHW格式是指按照通道数、高度和宽度的顺序排列图像尺寸的格式。例如，一张形状为3×256×256的RGB图像，在CHW格式中表示为[3, 256, 256]。**在计算机视觉和深度学习中如pytorch，通常使用CHW格式表示图像尺寸。**

例如，一个2x2的RGB图像在CHW格式下可能会以以下方式转为一维数组：

```py
原始的CHW格式表示为：
[
    [[R11, R12], [R21, R22]],
    [[G11, G12], [G21, G22]],
    [[B11, B12], [B21, B22]]
]
行优先扁平化后的一维表示为：
[R11, R12, R21, R22, G11, G12, G21, G22, B11, B12, B21, B22]
```

#### 为什么pytorch中transforms.ToTorch要把(H,W,C)的矩阵转为(C,H,W)? 

- 因为pytorch很多函数都是设计成假设你的输入是 （c，h，w）的格式，当然你如果不嫌麻烦的话可以每次要用这些函数的时候转成chw格式，但我想这会比你输入的时候就转成chw要麻烦很多。
- 至于为什么pytorch选择设计成chw而不是hwc（毕竟传统的读图片的函数opencv的cv2.imread或者sklearn的imread都是读成hwc的格式的）这点确实比较令初学者困惑。个人感觉是因为pytorch做矩阵加减乘除以及卷积等运算是需要调用cuda和cudnn的函数的，而这些接口都设成成chw格式了，故而pytorch为了方便起见也设计成chw格式了。
- 那新问题就来了，cuda和cudnn为什么设计成chw格式呢？我想这是由于涉及到图片操作的都是和卷积相关的，而内部做卷积运算的加速设计成chw在操作上会比hwc处理起来更容易，更快。题主如果想进一步了解可以google一下cudnn的卷积实现。



#### Torch将HWC格式转为CHW

```py
from PIL import Image
from torchvision.transforms import ToTensor
 
img = Image.open('image_path')
#产生的PIL_image格式数据的取值范围是[0,255]
#形状(shape)是[h, w, c]
#像素顺序是RGB
 
tensor = ToTensor()(PIL_img)
 	
# 或者
np_data = np.asarray(PIL_img)
tensor = ToTensor()(np_data)
```



## 6.4 神经网络架构

### 6.4.1 卷积Convolution





### 6.4.2 池化Pooling

在模糊的同时尽量的保留了原始图片的信息



### 6.4.3 非线性激活

    如果[神经元](https://so.csdn.net/so/search?q=神经元&spm=1001.2101.3001.7020)的输出是输入的线性函数，而线性函数之间的嵌套任然会得到线性函数。如果不加非线性函数处理，那么最终得到的仍然是线性函数。所以需要在神经网络中引入非线性激活函数。在[人工神经网络](https://so.csdn.net/so/search?q=人工神经网络&spm=1001.2101.3001.7020)中，激活函数决定是否需要传递信号。

![image-20230918104945081](./深度学习笔记.assets/image-20230918104945081.png)

![image-20230918104955808](./深度学习笔记.assets/image-20230918104955808.png)

​	通过对比非线性操作前后的图像，发现经过非线性激活函数处理之后的图像中的主要内容更突出了。
    ReLU函数处理自然语言效果更佳，Sigmoid函数处理图像效果更佳。

### 6.4.4 Dropout层

dropout是使指定概率的权重随机失活，作用是加快训练速度，测试的时候是不用的



## 6.5 熵

### 6.5.1 熵

那么熵的那些描述和解释(混乱程度，不确定性，惊奇程度，不可预测性，信息量等)代表了什么呢？

如果熵比较大(即平均编码长度较长)，意味着这一信息有较多的可能状态，相应的每个状态的可能性比较低；因此每当来了一个新的信息，我们很难对其作出准确预测，即有着比较大的混乱程度/不确定性/不可预测性。

并且当一个罕见的信息到达时，比一个常见的信息有着更多的信息量，因为它排除了别的很多的可能性，告诉了我们一个确切的信息。在天气的例子中，Rainy发生的概率为12.5%，当接收到该信息时，我们减少了87.5%的不确定性(Fine,Cloudy,Snow)；如果接收到Fine(50%)的消息，我们只减少了50%的不确定性。

### 6.5.2 [交叉熵（在分类问题中常见的损失函数）](https://blog.csdn.net/Yue_Zengying/article/details/117111510?ops_request_misc=&request_id=&biz_id=102&utm_term=%E4%BA%A4%E5%8F%89%E7%86%B5&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-9-117111510.142^v94^insert_down28v1&spm=1018.2226.3001.4187)

假设有两个机器学习模型对第一张照片分别作出了预测：Q1和Q2,而第一张照片的真实标签为[1,0,0,0,0]。![img](https://img-blog.csdnimg.cn/202105211147578.png#pic_center)

两个模型预测效果如何呢，可以分别计算下交叉熵：

<img src="https://img-blog.csdnimg.cn/20210521114815834.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1l1ZV9aZW5neWluZw==,size_16,color_FFFFFF,t_70#pic_center" alt="img" style="zoom:67%;" />

交叉熵对比了模型的预测结果和数据的真实标签，随着预测越来越准确，交叉熵的值越来越小，如果预测完全正确，交叉熵的值就为0。因此，训练分类模型时，可以使用交叉熵作为损失函数。

### 6.5.3 二分类交叉熵

在二分类模型中，标签只有是和否两种；这时，可以使用二分类交叉熵作为损失函数。假设数据集中只有猫和狗的照片，则交叉熵公式中只包含两种可能性：![img](https://img-blog.csdnimg.cn/202105211150414.jpg#pic_center)

而P(cat) = 1 - P(dog)
所以交叉熵可以表示为：
H(P,Q)=-P(cat)logQ(cat)-(1-P(cat))log(1-Q(cat))
使用如下定义：

![img](https://img-blog.csdnimg.cn/20210521115229397.png#pic_center)

二分类的交叉熵可以写作如下形式，看起来就熟悉多了。

![img](https://img-blog.csdnimg.cn/20210521115247963.png#pic_center)

## 6.6 [tensor中维度的理解](https://blog.csdn.net/AlexFaker/article/details/122526169?ops_request_misc=&request_id=&biz_id=102&utm_term=tensor%20%E7%BB%B4%E5%BA%A6&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-122526169.142^v94^insert_down28v1&spm=1018.2226.3001.4187)

```py
import torch

a = torch.randn(3, 4, 5)

print(a)

b = torch.cat((a,a), dim=0)
print(b)

c = torch.cat((a,a), dim=1)
print(c)

d = torch.cat((a,a), dim=2)
print(d)
```

```py
#输出 a
tensor([[[ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844]],

        [[ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552]],

        [[ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747]]])
# b = torch.cat((a,a), dim=0)  -> 这里的0维对应的是a的第一个3，最高维拼接
tensor([[[ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844]],

        [[ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552]],

        [[ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747]],

        [[ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844]],

        [[ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552]],

        [[ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747]]])
#c = torch.cat((a,a), dim=1)  -> 1维对应第二维，列拼接
tensor([[[ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844],
         [ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844]],

        [[ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552],
         [ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552]],

        [[ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747],
         [ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747]]])
#d = torch.cat((a,a), dim=2)  -> dim=2对应最低维，对应行拼接
tensor([[[ 0.0696,  0.6560,  0.2202, -1.0453,  0.9021,  0.0696,  0.6560,
           0.2202, -1.0453,  0.9021],
         [-0.1584, -0.9985, -1.1443, -1.4210, -0.4188, -0.1584, -0.9985,
          -1.1443, -1.4210, -0.4188],
         [ 1.4070,  2.9814, -2.1767,  1.0888,  0.9540,  1.4070,  2.9814,
          -2.1767,  1.0888,  0.9540],
         [ 0.3563, -0.7126,  0.2440, -0.1334, -1.0844,  0.3563, -0.7126,
           0.2440, -0.1334, -1.0844]],

        [[ 0.2997,  0.8679, -0.8382,  0.8501, -1.4777,  0.2997,  0.8679,
          -0.8382,  0.8501, -1.4777],
         [-0.2198, -0.0803, -0.4598, -0.0678,  0.1530, -0.2198, -0.0803,
          -0.4598, -0.0678,  0.1530],
         [-0.4743, -0.5473, -1.2885, -1.7756, -0.4839, -0.4743, -0.5473,
          -1.2885, -1.7756, -0.4839],
         [ 0.8187, -0.0167,  1.6490,  0.4461, -1.6552,  0.8187, -0.0167,
           1.6490,  0.4461, -1.6552]],

        [[ 1.3202, -0.8440,  0.1945,  0.3633,  0.2360,  1.3202, -0.8440,
           0.1945,  0.3633,  0.2360],
         [ 1.0943,  2.1030, -0.5662, -0.8079, -1.3058,  1.0943,  2.1030,
          -0.5662, -0.8079, -1.3058],
         [ 0.6459, -0.2108, -0.8447, -0.7190,  0.3210,  0.6459, -0.2108,
          -0.8447, -0.7190,  0.3210],
         [-0.1285,  1.5799,  0.1454,  1.6499, -1.3747, -0.1285,  1.5799,
           0.1454,  1.6499, -1.3747]]])

Process finished with exit code 0

```

## 6.7 学习速率Lr

太大训练不稳定，太小模型训练太慢，一般一开始设计大点的速率进行学习，学习到后面就采用一个比较小的速率进行学习



## 6.8 [迁移学习](https://blog.csdn.net/sikh_0529/article/details/126864397?ops_request_misc=&request_id=&biz_id=102&utm_term=%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-1-126864397.142^v94^insert_down28v1&spm=1018.2226.3001.4187)

迁移学习(Transfer Learning)是一种机器学习方法，就是把为任务 A 开发的模型作为初始点，重新使用在为任务 B 开发模型的过程中。迁移学习是通过从已学习的相关任务中转移知识来改进学习的新任务，虽然大多数机器学习算法都是为了解决单个任务而设计的，但是促进迁移学习的算法的开发是机器学习社区持续关注的话题。 迁移学习对人类来说很常见，例如，我们可能会发现学习识别苹果可能有助于识别梨，或者学习弹奏电子琴可能有助于学习钢琴。

找到目标问题的相似性，迁移学习任务就是从相似性出发，将旧领域(domain)学习过的模型应用在新领域上。




# 7 完整的模型训练套路
